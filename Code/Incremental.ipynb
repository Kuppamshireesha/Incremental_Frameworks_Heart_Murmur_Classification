{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataset(classCSVPath):\n",
    "    mfcc_df = pd.read_csv(classCSVPath)    \n",
    "    label_encoder = LabelEncoder()\n",
    "    mfcc_df[\"Label\"] = label_encoder.fit_transform(mfcc_df[\"Label\"])\n",
    "    \n",
    "    mfcc_features = mfcc_df.iloc[:, 3:].values  # Extract MFCC features\n",
    "    spectrogram_paths = mfcc_df[\"Spectrogram_Path\"].values\n",
    "    labels = mfcc_df[\"Label\"].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    mfcc_features = scaler.fit_transform(mfcc_features)\n",
    "    mfcc_features = mfcc_features.reshape(mfcc_features.shape[0], mfcc_features.shape[1], 1)\n",
    "    \n",
    "    labels_one_hot = to_categorical(labels)\n",
    "    X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc, train_indices, test_indices = train_test_split(\n",
    "        mfcc_features, labels_one_hot, range(len(labels)), test_size=0.3, random_state=37\n",
    "    )\n",
    "    \n",
    "    X_train_spectrogram_paths = spectrogram_paths[train_indices]\n",
    "    X_test_spectrogram_paths = spectrogram_paths[test_indices]\n",
    "    \n",
    "    def load_spectrogram_images(paths):\n",
    "        images = []\n",
    "        labels = []\n",
    "        BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../\"))  # Get base directory (parent of \"Code\")\n",
    "\n",
    "        for path in paths:\n",
    "            corrected_path = os.path.abspath(os.path.join(BASE_DIR, path))  # Convert relative path to absolute\n",
    "            if os.path.exists(corrected_path):\n",
    "                img = load_img(corrected_path, target_size=(224, 224), color_mode='rgb')\n",
    "                img = img_to_array(img) / 255.0\n",
    "                images.append(img)\n",
    "                labels.append(os.path.basename(os.path.dirname(corrected_path)))\n",
    "            else:\n",
    "                print(f\"Warning: Spectrogram not found at {corrected_path}\")  # Debugging print\n",
    "        \n",
    "        return np.array(images), labels\n",
    "\n",
    "    \n",
    "    X_train_spectrogram, spectrogram_train_labels = load_spectrogram_images(X_train_spectrogram_paths)\n",
    "    X_test_spectrogram, spectrogram_test_labels = load_spectrogram_images(X_test_spectrogram_paths)\n",
    "    \n",
    "    spectrogram_train_labels = label_encoder.transform(spectrogram_train_labels)\n",
    "    spectrogram_test_labels = label_encoder.transform(spectrogram_test_labels)\n",
    "    \n",
    "    y_train_spectrogram = to_categorical(spectrogram_train_labels)\n",
    "    y_test_spectrogram = to_categorical(spectrogram_test_labels)\n",
    "    \n",
    "    return X_train_mfcc, y_train_mfcc, X_test_mfcc, y_test_mfcc, X_train_spectrogram, y_train_spectrogram, X_test_spectrogram, y_test_spectrogram, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_audio_path = \"C:/Users/HP/Downloads/Project/Dataset/base_audio_mfcc_features_with_labels.csv\"\n",
    "X_train_mfcc, y_train_mfcc, X_test_mfcc, y_test_mfcc, X_train_spec, y_train_spec, X_test_spec, y_test_spec, label_encoder = SplitDataset(base_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample MFCC Features Shape: (144, 13, 1)\n",
      "Sample Spectrogram Image Shape: (144, 224, 224, 3)\n",
      "Sample Label One-Hot: [0. 1. 0. 0.]\n",
      "Sample Label One-Hot: [0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Print sample output\n",
    "print(f\"Sample MFCC Features Shape: {X_test_mfcc.shape}\")  # (samples, timesteps, features)\n",
    "print(f\"Sample Spectrogram Image Shape: {X_test_spec.shape}\")  # (samples, 224, 224, 3)\n",
    "print(f\"Sample Label One-Hot: {y_test_mfcc[12]}\")  # Example label\n",
    "print(f\"Sample Label One-Hot: {y_test_spec[12]}\")  # Example label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_audio_path = \"C:/Users/HP/Downloads/Project/Dataset/inc_audio_mfcc_features_with_labels.csv\"\n",
    "new_X_train_mfcc, new_y_train_mfcc, new_X_test_mfcc, new_y_test_mfcc, new_X_train_spec, new_y_train_spec, new_X_test_spec, new_y_test_spec, new_label_encoder = SplitDataset(inc_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample MFCC Features Shape: (96, 13, 1)\n",
      "Sample Spectrogram Image Shape: (96, 224, 224, 3)\n",
      "Sample Label One-Hot: [0. 0. 1. 0.]\n",
      "Sample Label One-Hot: [0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Print sample output\n",
    "print(f\"Sample MFCC Features Shape: {new_X_test_mfcc.shape}\")  # (samples, timesteps, features)\n",
    "print(f\"Sample Spectrogram Image Shape: {new_X_test_spec.shape}\")  # (samples, 224, 224, 3)\n",
    "print(f\"Sample Label One-Hot: {new_y_test_mfcc[12]}\")  # Example label\n",
    "print(f\"Sample Label One-Hot: {new_y_test_spec[12]}\")  # Example label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained MFCC models\n",
    "mfcc_model_1 = tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\MFCC_H5\\mfcc_BiLSTM_Model.h5\")\n",
    "mfcc_model_2 = tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\MFCC_H5\\mfcc_cnn_model.h5\")\n",
    "mfcc_model_3 = tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\MFCC_H5\\mfcc_LSTM_Model.h5\")\n",
    "\n",
    "mfcc_weights = [0.3, 0.2, 0.5] \n",
    "\n",
    "mfcc_models = [mfcc_model_1, mfcc_model_2, mfcc_model_3]\n",
    "\n",
    "spec_model_1 = tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\spec_mobilenet_model.h5\")\n",
    "spec_model_2 = tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\spectrogram_cnn_model.h5\")\n",
    "spec_model_3 = tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\spectrogram_resnet_model.h5\")\n",
    "\n",
    "spec_weights = [0.2, 0.2, 0.6]  \n",
    "\n",
    "spec_models = [spec_model_1, spec_model_2, spec_model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EWC:\n",
    "    def __init__(self, prior_model, data_samples, num_sample=30):\n",
    "        self.prior_model = prior_model\n",
    "        self.prior_weights = prior_model.get_weights()  # Store initial weights\n",
    "        self.num_sample = num_sample\n",
    "        self.data_samples = data_samples\n",
    "        self.fisher_matrix = self.compute_fisher()\n",
    "\n",
    "    def compute_fisher(self):\n",
    "        fisher_accum = [np.zeros_like(w) for w in self.prior_model.trainable_weights]  # Initialize Fisher matrix\n",
    "\n",
    "        for _ in tqdm(range(self.num_sample)):\n",
    "            idx = np.random.randint(self.data_samples.shape[0])\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = tf.nn.log_softmax(self.prior_model(np.array([self.data_samples[idx]])))  # Forward pass\n",
    "            grads = tape.gradient(logits, self.prior_model.trainable_weights)  # Compute gradients\n",
    "\n",
    "            for i in range(len(grads)):\n",
    "                if grads[i] is not None:  # Skip layers with no gradients\n",
    "                    fisher_accum[i] += np.square(grads[i].numpy())  # Compute Fisher per layer\n",
    "\n",
    "        fisher_accum = [f / self.num_sample for f in fisher_accum]  # Normalize Fisher matrix\n",
    "        return fisher_accum\n",
    "    \n",
    "    def get_fisher(self):\n",
    "        return self.fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEWC:\n",
    "    def __init__(self, optimizer, loss_fn, prior_weights=None, lambda_=0.1):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.prior_weights = prior_weights\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def train(self, model, train_data, train_labels, fisher_matrix, epochs=10):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            for i in range(len(train_data)):  # Iterate through indices\n",
    "                X = train_data[i: i+1]  # Get batch (single sample)\n",
    "                y = train_labels[i: i+1]  # Get corresponding label\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    pred = model(X)  # Forward pass\n",
    "                    loss = self.loss_fn(y, pred)  # Compute loss\n",
    "\n",
    "                    # Add EWC penalty\n",
    "                    ewc_loss = self.lambda_ * sum(\n",
    "                        tf.reduce_sum(f * tf.square(w - w_old))\n",
    "                        for f, w, w_old in zip(fisher_matrix, model.trainable_weights, self.prior_weights)\n",
    "                    )\n",
    "                    total_loss = loss + ewc_loss\n",
    "\n",
    "                grads = tape.gradient(total_loss, model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "\n",
    "    def compute_penalty_loss(self, model, fisher_matrix):\n",
    "        penalty = 0\n",
    "        for f, w, p in zip(fisher_matrix, model.get_weights(), self.prior_weights):\n",
    "            if f.shape == w.shape:  # Ensure Fisher matrix matches weight shape\n",
    "                penalty += tf.reduce_sum(f * tf.square(w - p))\n",
    "        return 0.5 * self.lambda_ * penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed training data shape: (1, 1, 224, 224, 224, 3)\n",
      "Expected input shape: (None, 224, 224, 3)\n",
      "New training data shape: (1, 1, 224, 224, 224, 3)\n",
      "X_train shape: (224, 13, 1)\n",
      "y_train shape: (224, 4)\n",
      "Epoch 1/2 - Loss: 0.4603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Loss: 0.2255\n",
      "✅ Updated MFCC model 'bilstm' saved.\n",
      "X_train shape: (224, 13, 1)\n",
      "y_train shape: (224, 4)\n",
      "Epoch 1/2 - Loss: 0.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Loss: 0.2868\n",
      "✅ Updated MFCC model 'lstm' saved.\n",
      "X_train shape: (1, 1, 224, 224, 224, 3)\n",
      "y_train shape: (224, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions 1 and 224 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m spec_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    101\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m TrainEWC(loss_fn\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(), lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_X_train_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_y_train_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfisher_spec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spec_save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated_spec_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Updated Spectrogram model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[61], line 69\u001b[0m, in \u001b[0;36mTrainEWC.train\u001b[1;34m(self, model, X_train, y_train, fisher_matrix, epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m fisher_matrix \u001b[38;5;241m=\u001b[39m adjust_fisher_shapes(fisher_matrix, model)\n\u001b[0;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn)  \u001b[38;5;66;03m# ✅ Ensure model is compiled\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     70\u001b[0m prior_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_weights()  \u001b[38;5;66;03m# Store initial weights for EWC\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:827\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:45\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     42\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mDimension(\n\u001b[0;32m     43\u001b[0m     tensor_shape\u001b[38;5;241m.\u001b[39mdimension_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_shape()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m---> 45\u001b[0m   \u001b[43mbatch_dim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimension_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mtensor_slice_dataset(\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors,\n\u001b[0;32m     51\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure),\n\u001b[0;32m     52\u001b[0m     is_files\u001b[38;5;241m=\u001b[39mis_files,\n\u001b[0;32m     53\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mSerializeToString())\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:303\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m    is_compatible_with).\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m--> 303\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are not compatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    304\u001b[0m                    (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions 1 and 224 are not compatible"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load all MFCC and Spectrogram models\n",
    "mfcc_models = {\n",
    "    \"bilstm\": tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\MFCC_H5\\mfcc_BiLSTM_Model.h5\"),\n",
    "    \"lstm\": tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\MFCC_H5\\mfcc_LSTM_Model.h5\"),\n",
    "}\n",
    "\n",
    "spec_models = {\n",
    "    \"mbnet\": tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\spec_mobilenet_model.h5\"),\n",
    "    \"cnn\": tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\spectrogram_cnn_model.h5\"),\n",
    "    \"resnet\": tf.keras.models.load_model(r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\spectrogram_resnet_model.h5\"),\n",
    "}\n",
    "\n",
    "# Dummy EWC function (Replace with actual EWC implementation)\n",
    "class EWC:\n",
    "    def __init__(self, model, data):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "\n",
    "    def get_fisher(self):\n",
    "        return [tf.ones_like(w) for w in self.model.trainable_weights]  # Simulated Fisher matrix\n",
    "\n",
    "# Compute Fisher matrices\n",
    "data_samples_mfcc = np.random.rand(100, *mfcc_models[\"bilstm\"].input_shape[1:])\n",
    "data_samples_spec = np.random.rand(100, *spec_models[\"cnn\"].input_shape[1:])\n",
    "# new_X_train_spec = np.expand_dims(new_X_train_spec, axis=0)\n",
    "# print(\"Fixed training data shape:\", new_X_train_spec.shape)\n",
    "\n",
    "# print(\"Expected input shape:\", spec_models[\"cnn\"].input_shape)\n",
    "# print(\"New training data shape:\", new_X_train_spec.shape)\n",
    "\n",
    "fisher_mfcc = {name: EWC(model, data_samples_mfcc).get_fisher() for name, model in mfcc_models.items()}\n",
    "fisher_spec = {name: EWC(model, data_samples_spec).get_fisher() for name, model in spec_models.items()}\n",
    "\n",
    "# Ensure save directories exist\n",
    "mfcc_save_dir = r\"C:\\Users\\HP\\Downloads\\Project\\Code\\MFCC_H5\\Updated\"\n",
    "spec_save_dir = r\"C:\\Users\\HP\\Downloads\\Project\\Code\\Spec_H5\\Updated\"\n",
    "os.makedirs(mfcc_save_dir, exist_ok=True)\n",
    "os.makedirs(spec_save_dir, exist_ok=True)\n",
    "\n",
    "# ✅ Adjust Fisher matrix shapes\n",
    "def adjust_fisher_shapes(fisher_matrix, model):\n",
    "    return [tf.broadcast_to(f, w.shape) if f.shape != w.shape else f for f, w in zip(fisher_matrix, model.trainable_weights)]\n",
    "\n",
    "for name in mfcc_models.keys():\n",
    "    fisher_mfcc[name] = adjust_fisher_shapes(fisher_mfcc[name], mfcc_models[name])\n",
    "\n",
    "for name in spec_models.keys():\n",
    "    fisher_spec[name] = adjust_fisher_shapes(fisher_spec[name], spec_models[name])\n",
    "\n",
    "# ✅ Fix: Use fresh optimizer per model\n",
    "class TrainEWC:\n",
    "    def __init__(self, loss_fn, lambda_=0.1):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lambda_ = lambda_  # Regularization strength\n",
    "\n",
    "    def train(self, model, X_train, y_train, fisher_matrix, epochs=10):\n",
    "        if len(X_train.shape) == 3:  # If still (224, 224, 3), add batch dim\n",
    "            X_train = np.expand_dims(X_train, axis=0)  # (1, 224, 224, 3)\n",
    "\n",
    "        print(\"X_train shape:\", X_train.shape)\n",
    "        print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam()  # ✅ Fresh optimizer for each model\n",
    "        fisher_matrix = adjust_fisher_shapes(fisher_matrix, model)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=self.loss_fn)  # ✅ Ensure model is compiled\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "        prior_weights = model.get_weights()  # Store initial weights for EWC\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for X, y in dataset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    pred = model(X, training=True)\n",
    "                    loss = self.loss_fn(y, pred)\n",
    "\n",
    "                    # ✅ Fix: Ensure EWC Loss uses correct shapes\n",
    "                    ewc_loss = self.lambda_ * sum(\n",
    "                        tf.reduce_sum(f * tf.square(w - w_old))\n",
    "                        for f, w, w_old in zip(fisher_matrix, model.trainable_weights, prior_weights)\n",
    "                    )\n",
    "\n",
    "                    total_loss = loss + ewc_loss\n",
    "\n",
    "                grads = tape.gradient(total_loss, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))  # ✅ Fresh optimizer applied here\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# Train & Save Updated MFCC Models\n",
    "for name, model in mfcc_models.items():\n",
    "    trainer = TrainEWC(loss_fn=tf.keras.losses.CategoricalCrossentropy(), lambda_=0.1)\n",
    "    trainer.train(model, new_X_train_mfcc, new_y_train_mfcc, fisher_mfcc[name], epochs=2)\n",
    "\n",
    "    model.save(os.path.join(mfcc_save_dir, f\"updated_mfcc_{name}.h5\"))\n",
    "    print(f\"✅ Updated MFCC model '{name}' saved.\")\n",
    "\n",
    "# Train & Save Updated Spectrogram Models\n",
    "for name, model in spec_models.items():\n",
    "    trainer = TrainEWC(loss_fn=tf.keras.losses.CategoricalCrossentropy(), lambda_=0.1)\n",
    "    trainer.train(model, new_X_train_spec, new_y_train_spec, fisher_spec[name], epochs=2)\n",
    "\n",
    "    model.save(os.path.join(spec_save_dir, f\"updated_spec_{name}.h5\"))\n",
    "    print(f\"✅ Updated Spectrogram model '{name}' saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
